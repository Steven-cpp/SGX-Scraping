# DTL SGX Scraping

è¿™æ˜¯ DTL Data Engineer çš„ä¸‹ä¸€é˜¶æ®µæµ‹è¯•ï¼Œä½¿ç”¨ Python åœ¨ SGX ç½‘ç«™ä¸Šä¸‹è½½æŒ‡å®šæ—¥æœŸçš„å†å²æ•°æ®ã€‚

## I. Requirement

Design a job to download the following files daily from the above website:

1. WEBPXTICK_DT-*.zip
2. TickData_structure.dat
3. TC_*.txt
4. TC_structure.dat

As is shown in the following snapshot, each file corresponds to each drop down options in *Time and Sales Historical Data*.

![image-20230306093101769](https://raw.githubusercontent.com/Steven-cpp/myPhotoSet/main/image-20230306093101769.png)

Send us a **.tar.gz** or **.zip** file that contains all the relevant files that you would like to submit.

ç”±äºéœ€æ±‚æ–‡æ¡£çš„è¯´æ˜éå¸¸ generalï¼Œè‡ªç”±åº¦éå¸¸å¤§ï¼Œå› æ­¤æˆ‘ç»“åˆå®é™…çš„ä½¿ç”¨éœ€æ±‚ï¼Œå°†ç¨‹åºçš„è§„çº¦è¿›ä¸€æ­¥çš„ç»†åŒ–ã€‚

### 1. Input

è¯¥ç¨‹åºå¯ä»¥ç”¨å‘½ä»¤è¡Œè¿è¡Œï¼Œå¹¶æŒ‡å®šé€šè¿‡ `config` æ–‡ä»¶æŒ‡å®šå‚æ•°ï¼Œè¿™äº›å‚æ•°åŒ…æ‹¬ï¼š

1. **ä¸‹è½½çš„æ•°æ®ç±»å‹ `type`**

   ALL, Both, Tick, Trade Cancellation, Data Structure

2. **æ—¶é—´èŒƒå›´ (if not DS file) `range`**

   æ”¯æŒä¸¤ç§æ–¹å¼æŒ‡å®šæ—¶é—´èŒƒå›´: 1) å¼€å§‹æ—¥æœŸ - [ç»“æŸæ—¥æœŸ]; 2) æœ€è¿‘ n ä¸ªäº¤æ˜“æ—¥

3. **ä¿å­˜çš„ä½ç½® `root_path`**

   æŒ‡å®šä¸‹è½½æ–‡ä»¶çš„ä¿å­˜æ ¹ä½ç½®ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•ã€‚

4. **çˆ¶æ–‡ä»¶å¤¹** `parent_dir`

   ä¼šåœ¨ `root_path` ç›®å½•ä¸‹æ–°å»ºæ–‡ä»¶å¤¹ `parent_dir` ä¿å­˜ã€‚å³æœ€ç»ˆçš„å­˜å‚¨è·¯å¾„ä½ `root_path/parent_dir`ã€‚

5. **æ—¥å¿—è¾“å‡ºä½ç½®** `output_loc`

   é»˜è®¤å°† `INFO` åŠä»¥ä¸Šè¾“å‡ºè‡³æ§åˆ¶å°ï¼Œå°† `DEGUG` åŠä»¥ä¸Šè¾“å‡ºè‡³æ–‡ä»¶ä¸­ã€‚

6. **æ˜¯å¦è‡ªåŠ¨é‡æ–°ä¸‹è½½** `auto_retry`

   åœ¨ä¸‹è½½ç»“æŸåï¼Œæ˜¯è‡ªåŠ¨é‡æ–°ä¸‹è½½å¤±è´¥çš„æ–‡ä»¶ï¼Œè¿˜æ˜¯è¯·æ±‚ç”¨æˆ·æ˜¯å¦éœ€è¦é‡æ–°ä¸‹è½½ã€‚é»˜è®¤ä¸º `false`ï¼Œé‡è¯• `max_retry` åè¿”å›ã€‚

7. **æœ€å¤§é‡æ–°ä¸‹è½½æ¬¡æ•° `max_retry`**

   ä¸‹è½½ä¸æˆåŠŸé‡è¯•çš„æ¬¡æ•°ã€‚

### 2. Output

è¯¥ç¨‹åºéœ€è¦ä½¿ç”¨ Python è‡ªå¸¦çš„ Logging module è¾“å‡ºç¨‹åºè¿è¡Œæ—¶çš„çŠ¶æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š

1. [INFO] è¿è¡Œé˜¶æ®µ

   å½“å‰æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡ï¼ŒåŠè¿›åº¦ã€‚æ€»å…±åˆ†ä¸ºä»¥ä¸‹ 3 ä¸ªè¿è¡Œé˜¶æ®µï¼š

   1. **åˆå§‹åŒ–é˜¶æ®µ**

      åŒ…æ‹¬å„ç§è¾“å…¥å‚æ•°çš„æ ¡éªŒã€ç±»å¯¹è±¡çš„åˆ›å»º

   2. **æ–‡ä»¶ä¸‹è½½é˜¶æ®µ**

      æ¥ç€ï¼Œå°±å¯ä»¥é€šè¿‡è¿™äº›å‚æ•°å‘é€ç›¸åº”çš„è¯·æ±‚ï¼Œä¸‹è½½æ–‡ä»¶ã€‚å¯¹äºä¸‹è½½çš„æ¯ä¸ªæ–‡ä»¶ï¼Œéƒ½ä¼šæ˜¾ç¤ºå…¶ä¸‹è½½è¿›åº¦ï¼Œå¹¶ä¸”è¿”å›å…¶æˆåŠŸæˆ–å¤±è´¥çš„ä¿¡æ¯ã€‚

      åœ¨æ‰€æœ‰ä»»åŠ¡ä¸‹è½½å®Œæˆåï¼Œæ˜¾ç¤º job summaryï¼Œç­‰å¾…ç”¨æˆ·æŒ‡ç¤ºæ˜¯å¦éœ€è¦é‡æ–°ä¸‹è½½æœªå®Œæˆçš„æ–‡ä»¶ã€‚

   3. **é‡æ–°ä¸‹è½½é˜¶æ®µ**

      å¦‚æœæœ‰æ–‡ä»¶ä¸‹è½½å¼‚å¸¸ï¼Œå¹¶ä¸”ç”¨æˆ·æŒ‡å®šé‡æ–°ä¸‹è½½ï¼Œåˆ™ä¼šè¿›å…¥è¯¥é˜¶æ®µã€‚é‡æ–°ä¸‹è½½è®°å½•ä¸‹æ¥çš„å¼‚å¸¸æ–‡ä»¶ã€‚

2. [Warning] æ–‡ä»¶ä¸‹è½½å¤±è´¥

   å…·ä½“è§ 3.2

3. [ERROR] config å‚æ•°é”™è¯¯

   å…·ä½“è§ 3.1

### 3. Exception Handling

æ­¤å¤–ï¼Œè¿˜è¦è€ƒè™‘è¯¥ç¨‹åºåœ¨è¿è¡Œæ—¶å¯èƒ½äº§ç”Ÿçš„å¼‚å¸¸æƒ…å†µï¼š

1. **ç”¨æˆ·äº¤äº’å¼‚å¸¸**

   ç”¨æˆ·è®¾ç½®çš„å‚æ•°å¯èƒ½æœ‰é—®é¢˜ï¼ŒåŒ…æ‹¬æ ¼å¼ä¸Šçš„ï¼Œä»¥åŠé€»è¾‘ä¸Šçš„ã€‚éœ€è¦ç¡®ä¿è¾“å…¥å‚æ•°åˆç†ã€æ­£ç¡®åï¼Œæ‰èƒ½è¿è¡Œç¨‹åºã€‚

2. **æ•°æ®ä¸‹è½½å¼‚å¸¸**

   åœ¨è¯·æ±‚ä¸‹è½½ url åï¼Œå¦‚æœæœåŠ¡å™¨è¿”å›çš„ä¸æ˜¯ `200` æˆ–è€…è¶…è¿‡äº† `timeout` ä»ç„¶æ²¡æœ‰è¿”å›ï¼Œå°±åº”å½“æŠ›å‡ºå¼‚å¸¸ã€‚

## II. Program Design

éµå¾ªè½¯ä»¶å·¥ç¨‹çš„ä¸€èˆ¬æµç¨‹ï¼Œåœ¨ç¡®è®¤éœ€æ±‚ä¹‹åï¼Œå°±å¯ä»¥å¯¹è½¯ä»¶è¿›è¡Œé¡¶å±‚è®¾è®¡äº†ã€‚

ä¸å¦¨å…ˆå®ç°è¯¥è½¯ä»¶çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œä¸‹è½½æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ–‡ä»¶ï¼Œå¹¶æ˜¾ç¤ºè¿›åº¦æ¡ã€‚åœ¨è¿™ä¸€æ ¸å¿ƒåŠŸèƒ½çš„åŸºç¡€ä¸Šï¼Œå†é€æ­¥æ‰©å……æ—¥å¿—è¾“å‡ºã€å‚æ•°è®¾å®šã€å¼‚å¸¸å¤„ç†è¿™äº›æ¨¡å—ã€‚

### 1. Core Module

æ ¸å¿ƒæ¨¡å—å‚è€ƒäº†[How To: Progress Bars for Python Downloads](https://www.alpharithms.com/progress-bars-for-python-downloads-580122/)ï¼Œé€šè¿‡ä½¿ç”¨ `request` å’Œ `tqdm` å®ç°äº†ä»æŒ‡å®š url ä»¥æµå¼ä¸‹è½½æ–‡ä»¶ï¼Œå¹¶ä¿å­˜è‡³ç”¨æˆ·æŒ‡å®šçš„ `root_path` ä¸‹çš„ `parent_dir` æ–‡ä»¶å¤¹ä¸­ã€‚

ç”±äºæ¯ä¸ªæ–‡ä»¶åœ¨æœåŠ¡å™¨ä¸Šéƒ½å¯¹åº”ç€ä¸€ä¸ªæ–‡ä»¶ IDï¼Œåˆå¯ä»¥é€šè¿‡è¯¥ ID ç”Ÿæˆè¯¥æ–‡ä»¶çš„ä¸‹è½½é“¾æ¥ï¼Œåªéœ€è¦é€šè¿‡ `request.get(url)` ä¾¿å¯ä¸‹è½½è¯¥æ–‡ä»¶ã€‚ä½†æ˜¯ç”¨æˆ·è¾“å…¥çš„æ˜¯ `date`ï¼Œè€Œä¸æ˜¯ `fileId`ï¼Œå› æ­¤éœ€è¦ç¡®å®šä¸¤è€…çš„æ˜ å°„å…³ç³»ã€‚

**date -> fileId**

æˆ‘å…ˆæ˜¯çŒœæµ‹ï¼Œåº”å½“æ˜¯äº¤æ˜“æ—¥ `buis_day` æ‰ä¼šäº§ç”Ÿè¯¥æ•°æ®æ–‡ä»¶ï¼Œæ‰€æœ‰çš„å‘¨æœ«ã€èŠ‚å‡æ—¥ `resr_day` åº”å½“éƒ½ä¼šè¢«è·³è¿‡ã€‚äºæ˜¯ï¼Œæˆ‘ä¸‹è½½äº†æœ€è¿‘ 100 å¤©çš„æ–‡ä»¶æ¥éªŒè¯è¿™ä¸€çŒœæƒ³ã€‚ä½†ç»è¿‡æˆ‘çš„è§‚å¯Ÿï¼Œåªæœ‰å‘¨æœ«çš„æ•°æ®ä¸å¯ç”¨ï¼Œå…¶å®ƒæ—¶é—´æ®µçš„æ•°æ®å‡å¯ç”¨ã€‚å› æ­¤ï¼Œåªéœ€è¦å–ä¸€ä¸ªæœ€æ—©çš„å‘¨ä¸€ä½œä¸º `BASE_DAY` ï¼Œå’Œå®ƒè®¡ç®— `passed_day`ï¼Œåˆ¤æ–­å–ä½™ 7 åï¼Œæ˜¯å¦ä¸º 5/6 (å‘¨æœ«) å³å¯ã€‚å¦‚æœæ˜¯çš„è¯ï¼Œç›´æ¥è·³è¿‡ã€‚

äºæ˜¯ï¼Œæˆ‘é€‰å®šäº†å¤§æ¦‚ 1000 ä¸ªäº¤æ˜“æ—¥ä¹‹å‰çš„å‘¨ä¸€ä½œä¸ºåŸºå‡†æ—¥æœŸï¼Œå°†å…¶å®šä¹‰ä¸º `BASE_DAY`ï¼Œç”¨æˆ·é€‰å®šçš„å¼€å§‹æ—¥æœŸä¸å¾—æ—©äº `BASE_DAY`ã€‚ä»è€Œå¯ä»¥é€šè¿‡ä»¥ä¸‹çš„å…¬å¼ï¼Œè®¡ç®—å‡ºä»»æ„æ—¥æœŸçš„ `deltaDay` ä¹Ÿå°±æ˜¯ `fileId`:

```python
days_passed = tarDay - BASE_DAY
n_rest = days_passed / 7 * 2
rem = days_passed % 7
if (rem < 5):
	delta_cur = delta_base + tarDay - BASE_DAY - n_rest
else:
  delta_cur = delta_base + tarDay - BASE_DAY - n_rest - rem + 4
```

ä½†äº‹å®ä¸Šï¼ŒæœåŠ¡å™¨ä¸­çš„ `delta_days` å­˜åœ¨å¼‚å¸¸ç‚¹ï¼Œå¹¶ä¸æ˜¯å¦‚ç†è®ºä¸­çš„é‚£ä¹ˆè¿ç»­ã€è§„å¾‹ã€‚ç»è¿‡é•¿è¾¾ 1 å°æ—¶çš„å°è¯•ï¼Œæˆ‘æ‰¾åˆ°äº† 2 ä¸ªå¼‚å¸¸ç‚¹ï¼Œå¦‚ä¸‹:

```python
BAD_DATES = [datetime(2020, 2, 1), datetime(2020, 11, 13)]
```

ç‰¹åˆ«åœ°ï¼Œ`2020-11-13` çš„ç†è®ºä½ç½®äº§ç”Ÿäº†ç©ºç¼ºï¼ŒID ä¸Šç§»äº† 2 ä½è‡³ `4768`ã€‚è€Œ `2020-02-01` å’Œ `2020-11-14` å‡æ˜¯å‘¨å…­ (éäº¤æ˜“æ—¥)ï¼Œå´æœ‰è®°å½•æ–‡ä»¶ã€‚

ä»è€Œï¼Œé’ˆå¯¹è¿™ä¸¤ä¸ªæ—¶é—´ç‚¹ï¼Œè¿˜éœ€è¦ç‰¹æ®Šå¤„ç†ã€‚ç”±äº `2020-02-01` æ˜¯å‘¨å…­ï¼Œä½†æ˜¯æœ‰äº¤æ˜“è®°å½•æ–‡ä»¶ã€‚äºæ˜¯ï¼Œåœ¨è¯¥å¤©åŠä¹‹åçš„æ—¥æœŸ `delta_days` éœ€è¦ +1ï¼Œè¡¨ç¤ºå¤šäº†ä¸€ä¸ªäº¤æ˜“æ—¥ã€‚è€Œå¯¹äº `2020-11-13`ï¼Œåˆäº§ç”Ÿäº†ç©ºç¼ºï¼Œåˆåœ¨å‘¨å…­æœ‰äº¤æ˜“è®°å½•ï¼Œå› æ­¤éœ€è¦ +2ï¼Œè¡¨ç¤ºå¤šäº†ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œè¿˜è¦è¡¥é½ä¸€ä¸ªäº¤æ˜“æ—¥çš„ç©ºç¼ºã€‚

æœ€ç»ˆï¼Œå¯ä»¥å¾—åˆ°ç”± `date` å‘ `deltaDays` çš„æ˜ å°„å‡½æ•°:

```python
def date2Deltadays(date: datetime) -> int:
  """
  Convert `date` to days(fileId) in SGX Derivitive hist data

  Noted
  -----
  It converts weekend to last Friday. So when weekend is set as start date, 
  it is implicitly perceived as last friday.
  """
  days_passed = (date - BASE_DATE).days
  n_rest = days_passed // 7 * 2
  rem = days_passed % 7
  if (rem < 5):
    delta_cur = BASE_DELTA + days_passed - n_rest
  else:
    delta_cur = BASE_DELTA + days_passed - n_rest - (rem - 4)

  if (date >= BAD_DATES[0]):
    delta_cur += 1
  if (date >= BAD_DATES[1]):
    delta_cur += 2
  return delta_cur
```

è‡³æ­¤ï¼Œæ ¸å¿ƒæ¨¡å—çš„å®ç°å·²ç»åŸºæœ¬å®Œæˆã€‚

### 2. Logging Module

æ¥ä¸‹æ¥ï¼Œä¸ºå½“å‰çš„ç¨‹åºæ·»åŠ æ—¥å¿—æ¨¡å—ã€‚To isolate and organize the logging logic, I decided to **refactor my functions in an object-oriented manner**. So that I can encapsulate logging logic within a class, controling when, where, and how logs are generated, and ensure that they are consistent across this scraper.

This can be implemented by simply importing `logging` module, and set up `logger` instance within the class from user specified config file. By default, the level of the file handler is set to `DEBUG` and the level of the console handler is set `INFO`, which means only levels >= `INFO` will show up on the console, while levels  >= `DEBUG` will be written to the log file.

A typical log file looks like the  following (with the year-month-date removed):

```python
10:14:06 - __main__ - INFO - =====   Scraper Initializing   =====
10:14:06 - __main__ - INFO - Logging module successfully configured
10:14:06 - root - INFO - checking download module configs...
10:14:06 - __main__ - INFO - Download module successfully configured
10:14:06 - __main__ - INFO - Done! Scraper Initialized
10:14:06 - __main__ - INFO - =====   Download History Data  =====
10:14:06 - __main__ - INFO - Job Overview:
10:14:06 - __main__ - INFO -    Date Range : from 2023-02-24 to 2023-03-09
10:14:06 - __main__ - INFO -    Data Type  : ALL
10:14:06 - __main__ - INFO -    Total Nums : 22
10:14:06 - __main__ - INFO -    Save Dir   : /Users/shiqi/Desktop/Projects/Seeking Job/Projects/histData
10:14:07 - __main__ - INFO - downloading structural data
10:14:07 - __main__ - INFO - 1/22: TC_structure.dat already downloaded
10:14:07 - __main__ - INFO - 2/22: TickData_structure.dat already downloaded
10:14:07 - __main__ - INFO - downloading history data
10:14:07 - __main__ - INFO - 3/22: WEBPXTICK_DT-20230224.zip already downloaded
10:14:07 - __main__ - INFO - 4/22: TC_20230224.txt already downloaded
...
10:14:07 - root - INFO - Successfully downloaded 22 files; 0 files failed
10:14:07 - root - INFO - Done! Complete downloading history data.
```

It consists of three stages, as is outlined in I.2

### 3. Error Recovery

This program may encounter two types of errors: configuration errors and download errors, as is outlined in I.3.

Configuration errors are classified as `ERROR` since they are unrecoverable until the user fixes the configuration. To prevent such errors, the program strictly checks all configurations during the scraper initialization stage, verifying both their type and logic. Once, invalid argument is detected, the program will send error message, and then exit.

While download errors are classified as `WARNING`, since in most cases, they are recoverable, resolved by re-downloading failed files. When such exception occurs, the program records the failed files and awaits user instruction to redownload them at the end of the job.

### 4. Optimization





> **ğŸš©ä¼˜åŒ–ç‚¹1: DS æ–‡ä»¶æ˜¯å¦ç›¸åŒ**
>
> - å¦‚æœç›¸åŒï¼Œåœ¨ç¨‹åºå¼€è·‘æ—¶ï¼Œä¸€æ¬¡æ€§è¯·æ±‚ DS æ–‡ä»¶ï¼Œç„¶åå¿½ç•¥ç”¨æˆ·çš„è¯·æ±‚ï¼Œå¹¶ç»™äºˆæç¤ºï¼›
> - å¦‚æœä¸åŒï¼Œéœ€è¦åˆ†å‡ºç›¸åŒçš„æ—¶é—´åŒºé—´ï¼Œå°†ä¸åŒç»“æ„çš„æ•°æ®æ–‡ä»¶æ”¾åœ¨ä¸åŒçš„æ–‡ä»¶å¤¹ä¸­ã€‚ä»¥æ–¹ä¾¿åæœŸçš„å¤„ç†å’Œåˆ†æã€‚
>
> -> å¯ä»¥å…ˆé€šè¿‡ä»¥ 50 å¤©ä¸ºé—´éš”è¿›è¡Œéå†ï¼Œä¸¤ä¸¤æ¯”è¾ƒæ–‡ä»¶å†…å®¹æ˜¯å¦ç›¸åŒã€‚å¦‚æœå®Œå…¨ç›¸åŒï¼Œå†åœ¨æ¯ä¸ª 50 å¤©çš„é—´éš”å†…ï¼ŒéšæœºæŠ½å– 10 å¤©è¿›è¡Œæ¯”è¾ƒã€‚å¦‚æœä»å®Œå…¨ç›¸åŒï¼Œåˆ™åŸºæœ¬å¯ä»¥ç¡®å®šæ‰€æœ‰çš„ DS æ–‡ä»¶å‡ç›¸åŒã€‚





































